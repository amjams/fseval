---
sidebar_position: 0
title: fseval.config.PipelineConfig
---

# PipelineConfig

<!-- Docusaurus -->
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

<!-- fseval -->

<!-- All the pipeline needs to run is a well-defined configuration. The requirement is that whatever is passed into [`run_pipeline`](/docs/running-experiments/running-first-experiment) is a `PipelineConfig` object.

The complete pipeline configuration is as follows: -->

```python
class fseval.config.PipelineConfig(
    dataset: DatasetConfig=MISSING,   
    cv: CrossValidatorConfig=MISSING,
    resample: ResampleConfig=MISSING,
    ranker: EstimatorConfig=MISSING,
    validator: EstimatorConfig=MISSING,
    storage: StorageConfig=MISSING,
    callbacks: Dict[str, Any]=field(default_factory=lambda: {}),
    metrics: Dict[str, Any]=field(default_factory=lambda: {}),
    n_bootstraps: int=1,
    n_jobs: Optional[int]=1,
    all_features_to_select: str="range(1, min(50, p) + 1)"
)
```

The complete configuration needed to run the fseval pipeline.

**Attributes**:

| | |
|---|---|
| `dataset` : [DatasetConfig](../DatasetConfig) | Determines the dataset to use for this experiment. |
| `cv` : [CrossValidatorConfig](../CrossValidatorConfig) | The CV method and split to use in this experiment. |
| `resample` : [ResampleConfig](../ResampleConfig) | Dataset resampling; e.g. with or without replacement. |
| `ranker` : [EstimatorConfig](../EstimatorConfig) | A Feature Ranker or Feature Selector. |
| `validator` : [EstimatorConfig](../EstimatorConfig) | Some estimator to validate the feature subsets. |
| `storage` : [StorageConfig](../StorageConfig) | A storage method used to store the fit estimators. |
| `callbacks` : Dict[str, Any] | Callbacks. Provide hooks for storing the config or results. |
| `metrics` : Dict[str, Any] | Metrics allow custom computation after any pipeline stage. |
| `n_bootstraps` : int | Amount of 'bootstraps' to run. A bootstrap means running the pipeline again but with a resampled (see `resample`) version of the dataset. This allows estimating stability, for example. |
| `n_jobs` : Optional[int] | Amount of CPU's to use for computing each bootstrap. This thus distributes the amount of bootstraps over CPU's. |
| `all_features_to_select` : str | Once the ranker has been fit, this determines the feature subsets to validate. By default, at most 50 subsets containing the highest ranked features are validated. |
| | |


Experiments can be configured in two ways.

1. Using **YAML** files stored in a directory
2. Using **Python** ([Structured Configs](https://hydra.cc/docs/tutorials/structured_config/intro/) in Hydra)

## Examples


<Tabs groupId="config-representation">
<TabItem value="yaml" label="YAML" default>

```yaml title="conf/my_config.yaml"
defaults:
  - _self_
  - base_pipeline_config
  - override dataset: iris
  - override validator: knn

n_bootstraps: 5
```

</TabItem>
<TabItem value="structured" label="Structured Config">

Any dataset can also be configured using Python code. Like so:

```python
from hydra.core.config_store import ConfigStore
from fseval.config import PipelineConfig
from typing import List, Any
from dataclasses import field

cs = ConfigStore.instance()

cs.store(name="my_config", node=PipelineConfig(
        defaults=field(
                default_factory=lambda: [
                        {"dataset": "iris"},
                        {"validator": "knn"}
                ]
        ),
        n_boostraps=5,
))
```

</TabItem>
</Tabs>